{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re\n",
    "import pickle\n",
    "from keras import models, regularizers, layers, optimizers, losses, metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, embeddings, Flatten\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in os.listdir(directory):\n",
    "    with open(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory):\n",
    "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "  pos_df[\"polarity\"] = 1\n",
    "  neg_df[\"polarity\"] = 0\n",
    "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_dataset(os.path.join('data', \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "test_df = load_dataset(os.path.join('data', \n",
    "                                      \"aclImdb\", \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data  (25000, 3)\n",
      "train_labels  (25000, 3)\n",
      "                                                sentence sentiment  polarity\n",
      "0      Joe Don Baker is one of a handful of actors wh...         8         1\n",
      "1      Interesting story about a soldier in a war who...         4         0\n",
      "2      Not only did they get the characters all wrong...         1         0\n",
      "3      When a friend gave me a boxed set of \"12 Amazi...         3         0\n",
      "4      The 1997 low-key indie dramedy Henry Fool woul...         7         1\n",
      "5      THE SECRET OF KELLS may be the most exquisite ...        10         1\n",
      "6      Uneven Bollywood drama. Karisma Kapoor is exce...         7         1\n",
      "7      I went into this movie hoping for the best. I ...         2         0\n",
      "8      While I totally disagree with one reviewer who...         3         0\n",
      "9      Worry not, Disney fans--this special edition D...         9         1\n",
      "10     Lisa is a hotel manager or owner and she gets ...         8         1\n",
      "11     what can i say?, ms Erika Eleniak is my favori...         9         1\n",
      "12     There seems to be a surprisingly high number o...         2         0\n",
      "13     Answer: despite that fact that this film was w...         7         1\n",
      "14     \"Panic\" is kind of a crime comedy-drama with W...         7         1\n",
      "15     I admit it's very silly, but I've practically ...        10         1\n",
      "16     I LOVE this movie! Beautifully funny and utter...         9         1\n",
      "17     This entertainingly tacky'n'trashy distaff \"De...         8         1\n",
      "18     I thought the original of this film was quaint...        10         1\n",
      "19     What is so taboo about love?! People seem to h...        10         1\n",
      "20     This is a must see for independant movie fans,...         8         1\n",
      "21     This is one of the more adorable episodes of t...        10         1\n",
      "22     \"The Screaming Skull\" opens with a warning and...         3         0\n",
      "23     I normally do not take the time to make commen...         2         0\n",
      "24     This norwegian movie is so crap, the actors ca...         3         0\n",
      "25     I am always wary of taking too instant a disli...         4         0\n",
      "26     It breaks my heart that this movie is not appr...         8         1\n",
      "27     Sorry everyone,,, I know this is supposed to b...         1         0\n",
      "28     Just caught it at the Toronto International Fi...         9         1\n",
      "29     Sorry this movie did not scare me it just anno...         4         0\n",
      "...                                                  ...       ...       ...\n",
      "24970  This is a CGI animated film based upon a Frenc...         8         1\n",
      "24971  It's a good movie if you plan to watch lots of...         3         0\n",
      "24972  Lynn Hollister, a small-town lawyer, travels t...         3         0\n",
      "24973  CQ could have been good, campy fun. But it com...         1         0\n",
      "24974  This remarkable film can be summed up very eas...        10         1\n",
      "24975  The kids, aged 7 to 14, got such a huge kick o...        10         1\n",
      "24976  In the area of movies based off of screenplays...        10         1\n",
      "24977  I can always tell when something is going to b...         1         0\n",
      "24978  Disregard the plot and enjoy Fred Astaire doin...         9         1\n",
      "24979  Not exactly a new story line, but this romanti...         4         0\n",
      "24980  \"Who Done It?\" contains many surefire laughs a...         9         1\n",
      "24981  Need a lesson in pure, abject failure?? Look n...         1         0\n",
      "24982  The Sunshine Boys is one of my favorite feel g...        10         1\n",
      "24983  People criticise Disney's animated features of...         9         1\n",
      "24984  Every year I watch hundreds of films, includin...         1         0\n",
      "24985  This film is harmless escapist fun. Something ...         8         1\n",
      "24986  This familiar story of an older man/younger wo...         8         1\n",
      "24987  I have copy of this on VHS, I think they (The ...         1         0\n",
      "24988  I saw this movie when it first came out. It wa...        10         1\n",
      "24989  An expedition led by hunky Captain Storm (Mark...         3         0\n",
      "24990  The Farrelly brothers, Bobby and Peter, are at...         7         1\n",
      "24991  This movie was excellent. I was not expecting ...        10         1\n",
      "24992  Back in 1994, I had a really lengthy vacation ...        10         1\n",
      "24993  If you're amused by straight-faced goings-on t...         9         1\n",
      "24994  An obvious b-grade effort to cash in on the Ho...         1         0\n",
      "24995  What can you say about this movie? It was not ...         4         0\n",
      "24996  It must have been excruciating to attend the d...         1         0\n",
      "24997  This program didn't do it for me, although I'm...         1         0\n",
      "24998  I watched this on an 8 hour flight and (presum...         7         1\n",
      "24999  I wasn't expecting the highest calibre of film...         7         1\n",
      "\n",
      "[25000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"train_data \", train_df.shape)\n",
    "print(\"train_labels \", test_df.shape)\n",
    "\n",
    "print(train_df)\n",
    "# print(\"_\"*100)\n",
    "# print(\"test_data \", X_test.shape)\n",
    "# print(\"test_labels \", y_test.shape)\n",
    "# print(\"_\"*100)\n",
    "# print(\"Maximum value of a word index \")\n",
    "# print(max([max(sequence) for sequence in X_train]))\n",
    "# print(\"Maximum length num words of review in train \")\n",
    "# print(max([len(sequence) for sequence in train_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 150\n",
    "train_sentence = train_df['sentence'].tolist()\n",
    "train_sentence = [' '.join(x.split()[:input_dim]) for x in train_sentence]\n",
    "train_sentence = np.array(train_sentence, dtype=object)\n",
    "train_polarity = train_df['polarity'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = test_df['sentence'].tolist()\n",
    "test_sentence = [' '.join(x.split()[:input_dim]) for x in test_sentence]\n",
    "test_sentence = np.array(test_sentence, dtype=object)\n",
    "test_polarity = test_df['polarity'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(num_words=1000,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ', lower=True, split=' ', char_level=False, oov_token=None, document_count=0)\n",
    "tok.fit_on_texts(train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = tok.texts_to_sequences(train_sentence)\n",
    "train_matrix = pad_sequences(train_matrix, maxlen=150, padding='post', truncating='post', value=0)\n",
    "# train_matrix = np.array(train_matrix, dtype=object)\n",
    "\n",
    "test_matrix = tok.texts_to_sequences(test_sentence)\n",
    "test_matrix = pad_sequences(test_matrix, maxlen=150, padding='post', truncating='post', value=0)\n",
    "# test_matrix = np.array(test_matrix, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_matrix = train_matrix[:,:,np.newaxis]\n",
    "# test_matrix = test_matrix[:,:,np.newaxis]\n",
    "# train_matrix.shape\n",
    "train_polarity = np.array(train_polarity)\n",
    "test_polarity = np.array(test_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_48 (Embedding)     (None, 150, 32)           32000     \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 4800)              0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 250)               1200250   \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 1,232,501\n",
      "Trainable params: 1,232,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(embeddings.Embedding(1000, 32, input_length=150))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/40\n",
      " - 5s - loss: 0.2586 - acc: 0.8952 - val_loss: 0.4826 - val_acc: 0.7987\n",
      "Epoch 2/40\n",
      " - 5s - loss: 0.1612 - acc: 0.9409 - val_loss: 0.6114 - val_acc: 0.7860\n",
      "Epoch 3/40\n",
      " - 5s - loss: 0.0798 - acc: 0.9746 - val_loss: 0.7790 - val_acc: 0.7752\n",
      "Epoch 4/40\n",
      " - 6s - loss: 0.0348 - acc: 0.9902 - val_loss: 1.0896 - val_acc: 0.7712\n",
      "Epoch 5/40\n",
      " - 5s - loss: 0.0173 - acc: 0.9955 - val_loss: 1.3058 - val_acc: 0.7668\n",
      "Epoch 6/40\n",
      " - 6s - loss: 0.0098 - acc: 0.9974 - val_loss: 1.4436 - val_acc: 0.7724\n",
      "Epoch 7/40\n",
      " - 5s - loss: 0.0068 - acc: 0.9980 - val_loss: 1.5578 - val_acc: 0.7670\n",
      "Epoch 8/40\n",
      " - 6s - loss: 0.0054 - acc: 0.9986 - val_loss: 1.6428 - val_acc: 0.7696\n",
      "Epoch 9/40\n",
      " - 5s - loss: 0.0041 - acc: 0.9988 - val_loss: 1.7846 - val_acc: 0.7675\n",
      "Epoch 10/40\n",
      " - 6s - loss: 0.0032 - acc: 0.9993 - val_loss: 1.9270 - val_acc: 0.7642\n",
      "Epoch 11/40\n",
      " - 5s - loss: 0.0034 - acc: 0.9990 - val_loss: 1.9166 - val_acc: 0.7674\n",
      "Epoch 12/40\n",
      " - 5s - loss: 0.0030 - acc: 0.9989 - val_loss: 1.9631 - val_acc: 0.7656\n",
      "Epoch 13/40\n",
      " - 5s - loss: 0.0018 - acc: 0.9996 - val_loss: 2.0142 - val_acc: 0.7666\n",
      "Epoch 14/40\n",
      " - 5s - loss: 0.0017 - acc: 0.9996 - val_loss: 2.0966 - val_acc: 0.7697\n",
      "Epoch 15/40\n",
      " - 5s - loss: 0.0022 - acc: 0.9993 - val_loss: 2.1011 - val_acc: 0.7683\n",
      "Epoch 16/40\n",
      " - 5s - loss: 0.0018 - acc: 0.9994 - val_loss: 2.1728 - val_acc: 0.7658\n",
      "Epoch 17/40\n",
      " - 5s - loss: 0.0018 - acc: 0.9995 - val_loss: 2.2826 - val_acc: 0.7641\n",
      "Epoch 18/40\n",
      " - 5s - loss: 0.0015 - acc: 0.9995 - val_loss: 2.2261 - val_acc: 0.7648\n",
      "Epoch 19/40\n",
      " - 5s - loss: 0.0010 - acc: 0.9997 - val_loss: 2.3106 - val_acc: 0.7639\n",
      "Epoch 20/40\n",
      " - 5s - loss: 0.0012 - acc: 0.9998 - val_loss: 2.3479 - val_acc: 0.7624\n",
      "Epoch 21/40\n",
      " - 5s - loss: 0.0015 - acc: 0.9996 - val_loss: 2.3840 - val_acc: 0.7634\n",
      "Epoch 22/40\n",
      " - 5s - loss: 0.0012 - acc: 0.9996 - val_loss: 2.4414 - val_acc: 0.7584\n",
      "Epoch 23/40\n",
      " - 5s - loss: 5.9641e-04 - acc: 0.9998 - val_loss: 2.4576 - val_acc: 0.7666\n",
      "Epoch 24/40\n",
      " - 5s - loss: 9.1608e-04 - acc: 0.9998 - val_loss: 2.4656 - val_acc: 0.7594\n",
      "Epoch 25/40\n",
      " - 5s - loss: 7.5419e-04 - acc: 0.9997 - val_loss: 2.5007 - val_acc: 0.7594\n",
      "Epoch 26/40\n",
      " - 5s - loss: 6.6582e-04 - acc: 0.9998 - val_loss: 2.5329 - val_acc: 0.7601\n",
      "Epoch 27/40\n",
      " - 5s - loss: 6.9511e-04 - acc: 0.9998 - val_loss: 2.5182 - val_acc: 0.7597\n",
      "Epoch 28/40\n",
      " - 5s - loss: 7.9650e-04 - acc: 0.9998 - val_loss: 2.5884 - val_acc: 0.7597\n",
      "Epoch 29/40\n",
      " - 5s - loss: 5.8213e-04 - acc: 0.9998 - val_loss: 2.5900 - val_acc: 0.7610\n",
      "Epoch 30/40\n",
      " - 6s - loss: 0.0016 - acc: 0.9995 - val_loss: 2.5519 - val_acc: 0.7606\n",
      "Epoch 31/40\n",
      " - 5s - loss: 8.4751e-04 - acc: 0.9996 - val_loss: 2.6075 - val_acc: 0.7584\n",
      "Epoch 32/40\n",
      " - 5s - loss: 2.6517e-04 - acc: 0.9999 - val_loss: 2.6526 - val_acc: 0.7597\n",
      "Epoch 33/40\n",
      " - 5s - loss: 2.0537e-04 - acc: 0.9999 - val_loss: 2.7014 - val_acc: 0.7626\n",
      "Epoch 34/40\n",
      " - 5s - loss: 4.4855e-04 - acc: 0.9998 - val_loss: 2.6956 - val_acc: 0.7603\n",
      "Epoch 35/40\n",
      " - 5s - loss: 1.6303e-04 - acc: 0.9999 - val_loss: 2.7231 - val_acc: 0.7600\n",
      "Epoch 36/40\n",
      " - 6s - loss: 3.6887e-04 - acc: 0.9998 - val_loss: 2.8273 - val_acc: 0.7547\n",
      "Epoch 37/40\n",
      " - 5s - loss: 1.4689e-04 - acc: 1.0000 - val_loss: 2.7801 - val_acc: 0.7547\n",
      "Epoch 38/40\n",
      " - 5s - loss: 6.6861e-04 - acc: 0.9996 - val_loss: 2.7722 - val_acc: 0.7582\n",
      "Epoch 39/40\n",
      " - 5s - loss: 1.7332e-04 - acc: 1.0000 - val_loss: 2.8289 - val_acc: 0.7534\n",
      "Epoch 40/40\n",
      " - 5s - loss: 2.7512e-04 - acc: 0.9998 - val_loss: 2.8552 - val_acc: 0.7546\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_matrix, train_polarity, validation_data=(test_matrix, test_polarity), epochs=40, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 85us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.41619171557426454, 0.81024]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_matrix,test_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('models/dl_arch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tok, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
